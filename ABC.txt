import requests
from bs4 import BeautifulSoup
import csv
import time

# Replace these with your login credentials
login_url = 'https://example.com/login'
data_url = 'https://example.com/data?page='  # Assuming page number is part of the URL
username = 'your_username'
password = 'your_password'

# Start a session to handle login
session = requests.Session()

# Login payload (customize based on the actual form data needed)
login_payload = {
    'username': username,
    'password': password,
    # Include any other fields required by the login form
}

# Step 1: Login
response = session.post(login_url, data=login_payload)
if response.status_code == 200:
    print("Logged in successfully.")
else:
    print("Login failed.")
    exit()

# Step 2: Scrape paginated data
# Open a CSV file to write the data
with open('data.csv', 'w', newline='', encoding='utf-8') as csvfile:
    csvwriter = csv.writer(csvfile)
    
    # Fetch the first page to get the column headers (Assuming table headers exist)
    page = 1
    response = session.get(f"{data_url}{page}")
    soup = BeautifulSoup(response.content, 'html.parser')

    # Extract column headers (customize based on the structure of your table)
    headers = [header.text.strip() for header in soup.select('table th')]
    if headers:
        csvwriter.writerow(headers)  # Write headers to CSV
    else:
        print("Failed to extract headers. Check table structure.")
        exit()

    # Loop through pages to get the data
    while True:
        # Fetch the data for the current page
        page_url = f"{data_url}{page}"
        response = session.get(page_url)
        if response.status_code != 200:
            print(f"Failed to load page {page}.")
            break

        # Parse the HTML page (customize selectors as needed)
        soup = BeautifulSoup(response.content, 'html.parser')

        # Extract table rows (customize based on website structure)
        rows = soup.select('table tr')
        if not rows:
            print(f"No more data on page {page}.")
            break

        # Extract and write data from each row
        for row in rows[1:]:  # Skip header row if it's part of the selection
            columns = row.find_all('td')
            if columns:  # Ensure it's a data row
                csvwriter.writerow([col.text.strip() for col in columns])

        # Print progress
        print(f"Scraped page {page}.")
        page += 1

        # Optionally add a delay to avoid overwhelming the server
        time.sleep(1)

print("Data extraction completed.")
