import pandas as pd

# Specify the file path
file_path = 'your_large_file.csv'

# Specify the chunk size (number of rows per chunk)
chunk_size = 10000  # Adjust as needed

# Create an empty list to hold the chunks
chunks = []

# Create an iterator that reads the CSV file in chunks
chunk_iterator = pd.read_csv(file_path, chunksize=chunk_size)

# Process each chunk
for chunk in chunk_iterator:
    # Perform operations on the chunk if needed
    # For example, filtering or transforming data

    # Append the processed chunk to the list
    chunks.append(chunk)

# Concatenate all chunks into a single DataFrame
full_df = pd.concat(chunks, ignore_index=True)

# Now you can work with the full DataFrame
print(full_df.shape)
print(full_df.head())
